#!/usr/bin/env python3
import argparse
import subprocess
import time
import sys
from typing import Optional, List, Dict, Any

def parse_args():
    parser = argparse.ArgumentParser(description="CIAI-Wrangler: Slurm Job Orchestration Tool")
    parser.add_argument("config_file", help="YAML configuration file path.")
    return parser.parse_args()

def read_job_scripts(job_file):
    with open(job_file) as f:
        jobs = [line.strip() for line in f if line.strip()]
    return jobs

class Logger:
    def __init__(self, logfile=None):
        self.logfile = logfile
        if logfile:
            self.f = open(logfile, 'w')
        else:
            self.f = None

    def log(self, msg):
        from datetime import datetime
        ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        line = f"[{ts}] {msg}"
        print(line)
        if self.f:
            self.f.write(line+'\n')
            self.f.flush()

    def close(self):
        if self.f:
            self.f.close()

def load_config(path: str) -> Dict[str, Any]:
    try:
        with open(path, 'r', encoding='utf-8') as f:
            text = f.read()
    except Exception as e:
        print(f"Error: failed to read config file '{path}': {e}", file=sys.stderr)
        sys.exit(2)

    try:
        import yaml  # type: ignore
    except ImportError:
        print("Error: PyYAML is required. Install with 'pip install pyyaml' and try again.", file=sys.stderr)
        sys.exit(2)

    try:
        data = yaml.safe_load(text)
    except Exception as e:
        print(f"Error: failed to parse YAML config: {e}", file=sys.stderr)
        sys.exit(2)

    if not isinstance(data, dict):
        print("Config error: top-level YAML must be a mapping (dictionary).", file=sys.stderr)
        sys.exit(2)

    errors: List[str] = []
    jobs = data.get('jobs')
    if not isinstance(jobs, list) or not all(isinstance(x, str) and x.strip() for x in jobs):
        errors.append("Config error: 'jobs' must be a non-empty list of strings.")
    max_queue = data.get('max_queue', 3)
    if not isinstance(max_queue, int) or max_queue <= 0:
        errors.append("Config error: 'max_queue' must be a positive integer.")
    log_file = data.get('log_file', None)
    if log_file is not None and not isinstance(log_file, str):
        errors.append("Config error: 'log_file' must be a string or null.")

    if errors:
        for err in errors:
            print(err, file=sys.stderr)
        sys.exit(2)

    return {
        'jobs': [str(j).strip() for j in jobs],
        'max_queue': int(max_queue),
        'log_file': log_file,
    }

def submit_job(script):
    try:
        result = subprocess.run(["sbatch", script], capture_output=True, text=True)
        if result.returncode != 0:
            return None, result.stderr.strip()
        # Output like: "Submitted batch job 12345"
        for line in result.stdout.strip().splitlines():
            if line.startswith("Submitted batch job"):
                jobid = line.strip().split()[-1]
                return jobid, None
        return None, "Unrecognized sbatch output: " + result.stdout
    except Exception as e:
        return None, str(e)


# ---
# WARNING: DISABLED sacct polling due to SLURM issues. Only squeue is used. If a job id is missing in squeue, we assume completion (success or failure undetermined).
def check_job_status(jobid):
    # Returns: 'queued', 'running', 'completed' (cannot distinguish failed/completed)
    squeue = subprocess.run([
        "squeue", "-j", str(jobid), "-h", "-o", "%T"
    ], capture_output=True, text=True)
    if squeue.returncode == 0:
        state = squeue.stdout.strip().split()
        if state:
            sq_state = state[0].upper()
            if sq_state == "PENDING":
                return "queued"
            elif sq_state == "RUNNING":
                return "running"
    # If not in squeue, assume it's completed (status unknown)
    return "completed"

def cluster_health_check(log):
    import shutil
    #required_cmds = ["sbatch", "squeue", "sacct"]
    required_cmds = ["sbatch", "squeue"]
    log.log("Performing cluster health check...")
    all_ok = True
    for cmd in required_cmds:
        path = shutil.which(cmd)
        if path:
            log.log(f"{cmd} OK")
        else:
            log.log(f"{cmd} FAIL: '{cmd}' not found in PATH. Please ensure Slurm CLI tools are installed and accessible.")
            all_ok = False

    # Advanced check: sacct basic call (to check if slurmdbd is accessible)
    #import subprocess
    #sacct_path = shutil.which("sacct")
    #if sacct_path:
    #    try:
    #        test = subprocess.run(["sacct", "-n", "-o", "JobID", "-X", "--help"], capture_output=True, text=True)
    #        # try a simple call that would not require job history but hit the DBD
    #        test2 = subprocess.run(["sacct", "-n", "-o", "JobID", "-X", "-S", "now-1hour"], capture_output=True, text=True)
    #        if test2.returncode != 0:
    #            msg = test2.stderr.strip() or 'Unknown error'
    #            log.log(f"sacct FAIL: sacct command is present but does not work correctly: {msg}")
    #            all_ok = False
    #    except Exception as e:
    #        log.log(f"sacct FAIL: exception when running sacct: {str(e)}")
    #        all_ok = False
    return all_ok

def main():
    args = parse_args()
    cfg = load_config(args.config_file)
    jobs = list(cfg['jobs'])
    log = Logger(cfg['log_file'])

    if not cluster_health_check(log):
        log.log("Cluster health check failed. Exiting.")
        log.close()
        sys.exit(1)

    running_jobs = []  # list of (jobid, script)
    to_submit = list(jobs)
    jobid_to_script = {}
    job_last_status = {}

    # Start up to -q jobs
    while to_submit and len(running_jobs) < cfg['max_queue']:
        script = to_submit.pop(0)
        jobid, err = submit_job(script)
        if jobid:
            jobid_to_script[jobid] = script
            log.log(f"Job {script} started with ID {jobid} (queued)")
            running_jobs.append((jobid, script))
            job_last_status[jobid] = "queued"
        else:
            log.log(f"Job {script} failed to submit: {err}")

    finished_jobs = set()

    while running_jobs or to_submit:
        time.sleep(5)
        still_running = []
        for jobid, script in running_jobs:
            status = check_job_status(jobid)
            prev_status = job_last_status.get(jobid)
            # Only log a state when changed (except for completion/fail)
            if status in ("queued", "running"):
                if status != prev_status:
                    log.log(f"Job {script} with ID {jobid} {status}")
                still_running.append((jobid, script))
                job_last_status[jobid] = status
            elif status in ("completed", "failed"):
                log.log(f"Job {script} with ID {jobid} {status}")
                finished_jobs.add(jobid)
                job_last_status[jobid] = status
                # Start next if any remain
                if to_submit:
                    next_script = to_submit.pop(0)
                    next_jobid, err = submit_job(next_script)
                    if next_jobid:
                        jobid_to_script[next_jobid] = next_script
                        log.log(f"Job {next_script} started with ID {next_jobid} (queued)")
                        still_running.append((next_jobid, next_script))
                        job_last_status[next_jobid] = "queued"
                    else:
                        log.log(f"Job {next_script} failed to submit: {err}")
            else:
                # If status is None, recheck later
                still_running.append((jobid, script))
        running_jobs = still_running

    log.close()

if __name__ == "__main__":
    main()
